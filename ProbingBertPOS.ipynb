{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing Bert part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "TRAIN_FILE = \"en_ewt-ud-train.conllu\"\n",
    "EVAL_FILE = \"en_ewt-ud-dev.conllu\"\n",
    "TEST_FILE = \"en_ewt-ud-test.conllu\"\n",
    "\n",
    "for filename in [ TRAIN_FILE, EVAL_FILE, TEST_FILE ]:\n",
    "  urllib.request.urlretrieve('https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/' + filename, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Al', 'NNP'), ('-', 'HYPH'), ('Zaman', 'NNP'), (':', ':'), ('American', 'JJ'), ('forces', 'NNS'), ('killed', 'VBD'), ('Shaikh', 'NNP'), ('Abdullah', 'NNP'), ('al', 'NNP'), ('-', 'HYPH'), ('Ani', 'NNP'), (',', ','), ('the', 'DT'), ('preacher', 'NN'), ('at', 'IN'), ('the', 'DT'), ('mosque', 'NN'), ('in', 'IN'), ('the', 'DT'), ('town', 'NN'), ('of', 'IN'), ('Qaim', 'NNP'), (',', ','), ('near', 'IN'), ('the', 'DT'), ('Syrian', 'JJ'), ('border', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import conllu\n",
    "\n",
    "def load_conllu(filename):\n",
    "  with open(filename, encoding=\"utf-8\") as fp:\n",
    "    data = conllu.parse(fp.read())\n",
    "  sentences = [[token['form'] for token in sentence] for sentence in data]\n",
    "  taggings = [[token['xpos'] for token in sentence] for sentence in data]\n",
    "  return sentences, taggings\n",
    "\n",
    "train_sentences, train_labels = load_conllu(TRAIN_FILE)\n",
    "eval_sentences, eval_labels = load_conllu(EVAL_FILE)\n",
    "test_sentences, test_labels = load_conllu(TEST_FILE)\n",
    "print(list(zip(train_sentences[0], train_labels[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'interesting',\n",
       " 'for',\n",
       " 'su',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##ure']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.tokenize(\"This is interesting for suuuuuuuuuuuuuuuuuure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'has', 'been', 'talk', 'that', 'the', 'night', 'cu', '##rf', '##ew', 'might', 'be', 'implemented', 'again', '.']\n",
      "['EX', 'VBZ', 'VBN', 'NN', 'IN', 'DT', 'NN', '<pad>', '<pad>', 'NN', 'MD', 'VB', 'VBN', 'RB', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def align_to_bert_tokenization(sentences, labels):\n",
    "    tokenized_sentences = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    for s, l in zip(sentences, labels):\n",
    "        tokenized_sentence = tokenizer.tokenize(' '.join(s))\n",
    "        aligned_label = []\n",
    "        current_word = ''\n",
    "        i = 0\n",
    "        for token in tokenized_sentence:\n",
    "            current_word += re.sub(r'^##', '', token)\n",
    "            s[i] = s[i].replace('\\xad', '')\n",
    "            \n",
    "            assert token == '[UNK]' or s[i].startswith(current_word)\n",
    "\n",
    "            if token == '[UNK]' or s[i] == current_word:\n",
    "                current_word = ''\n",
    "                aligned_label.append(l[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                aligned_label.append('<pad>')\n",
    "        \n",
    "        assert len(tokenized_sentence) == len(aligned_label)\n",
    "\n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "        aligned_labels.append(aligned_label)\n",
    "    \n",
    "    return tokenized_sentences, aligned_labels\n",
    "\n",
    "\n",
    "train_bert_tokenized_sentences, train_aligned_taggings = align_to_bert_tokenization(train_sentences, train_labels)\n",
    "eval_bert_tokenized_sentences, valid_aligned_taggings = align_to_bert_tokenization(eval_sentences, eval_labels)\n",
    "test_bert_tokenized_sentences, test_aligned_taggings = align_to_bert_tokenization(test_sentences, test_labels)\n",
    "\n",
    "print(train_bert_tokenized_sentences[42])\n",
    "print(train_aligned_taggings[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\ProbingBertPOS.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000008?line=16'>17</a>\u001b[0m     taggings_ids\u001b[39m.\u001b[39mappend(tagging_tensor\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000008?line=17'>18</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m sentences_ids, taggings_ids\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000008?line=19'>20</a>\u001b[0m train_sentences_ids, train_taggings_ids \u001b[39m=\u001b[39m convert_to_ids(train_bert_tokenized_sentences, train_aligned_taggings)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000008?line=20'>21</a>\u001b[0m eval_sentences_ids, eval_taggings_ids \u001b[39m=\u001b[39m convert_to_ids(eval_bert_tokenized_sentences, valid_aligned_taggings)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000008?line=21'>22</a>\u001b[0m test_sentences_ids, test_taggings_ids \u001b[39m=\u001b[39m convert_to_ids(test_bert_tokenized_sentences, test_aligned_taggings)\n",
      "\u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\ProbingBertPOS.ipynb Cell 6'\u001b[0m in \u001b[0;36mconvert_to_ids\u001b[1;34m(sentences, taggings)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000008?line=12'>13</a>\u001b[0m   sentence_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(tokenizer\u001b[39m.\u001b[39mconvert_tokens_to_ids([\u001b[39m'\u001b[39m\u001b[39m[CLS]\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m sentence \u001b[39m+\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mSEP\u001b[39m\u001b[39m'\u001b[39m]))\u001b[39m.\u001b[39mlong()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000008?line=13'>14</a>\u001b[0m   tagging_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m [label_vocab[tag] \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagging] \u001b[39m+\u001b[39m [\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mlong()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000008?line=15'>16</a>\u001b[0m   sentences_ids\u001b[39m.\u001b[39mappend(sentence_tensor\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000008?line=16'>17</a>\u001b[0m   taggings_ids\u001b[39m.\u001b[39mappend(tagging_tensor\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000008?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m sentences_ids, taggings_ids\n",
      "File \u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\env\\lib\\site-packages\\torch\\cuda\\__init__.py:210\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=205'>206</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=206'>207</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=207'>208</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=208'>209</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=209'>210</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=210'>211</a>\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=211'>212</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=212'>213</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')\n",
    "\n",
    "import collections\n",
    "\n",
    "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
    "label_vocab['<pad>'] = 0\n",
    "\n",
    "def convert_to_ids(sentences, taggings):\n",
    "  sentences_ids = []\n",
    "  taggings_ids = []\n",
    "  for sentence, tagging in zip(sentences, taggings):\n",
    "    sentence_tensor = torch.tensor(tokenizer.convert_tokens_to_ids(['[CLS]'] + sentence + ['SEP'])).long()\n",
    "    tagging_tensor = torch.tensor([0] + [label_vocab[tag] for tag in tagging] + [0]).long()\n",
    "\n",
    "    sentences_ids.append(sentence_tensor.to(device))\n",
    "    taggings_ids.append(tagging_tensor.to(device))\n",
    "  return sentences_ids, taggings_ids\n",
    "\n",
    "train_sentences_ids, train_taggings_ids = convert_to_ids(train_bert_tokenized_sentences, train_aligned_taggings)\n",
    "eval_sentences_ids, eval_taggings_ids = convert_to_ids(eval_bert_tokenized_sentences, valid_aligned_taggings)\n",
    "test_sentences_ids, test_taggings_ids = convert_to_ids(test_bert_tokenized_sentences, test_aligned_taggings)\n",
    "\n",
    "print(train_sentences_ids[42])\n",
    "print(train_taggings_ids[42])\n",
    "print('num labels:', len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PosTaggingDataset(Dataset):\n",
    "  def __init__(self, sentences, taggings):\n",
    "    assert len(sentences) == len(taggings)\n",
    "    self.sentences = sentences\n",
    "    self.taggings = taggings\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    return self.sentences[i], self.taggings[i]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(items):\n",
    "  max_len = max(len(item[0]) for item in items)\n",
    "\n",
    "  sentences = torch.zeros((len(items), max_len), device=items[0][0].device).long().to(device)\n",
    "  taggings = torch.zeros((len(items), max_len)).long().to(device)\n",
    "\n",
    "  for i, (sentence, tagging) in enumerate(items):\n",
    "    sentences[i][0:len(sentence)] = sentence\n",
    "    taggings[i][0:len(tagging)] = tagging\n",
    "\n",
    "  return sentences, taggings\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(PosTaggingDataset(train_sentences_ids, train_taggings_ids), batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "eval_loader = DataLoader(PosTaggingDataset(eval_sentences_ids, eval_taggings_ids), batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(PosTaggingDataset(test_sentences_ids, test_taggings_ids), batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 52])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearProbeRandom(nn.Module):\n",
    "  def __init__(self, num_labels):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(tokenizer.vocab_size, 768)\n",
    "    self.probe = nn.Linear(768, num_labels)\n",
    "    self.to(device)\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.probe.parameters()\n",
    "  \n",
    "  def forward(self, sentences):\n",
    "    with torch.no_grad():\n",
    "      word_rep = self.embedding(sentences)\n",
    "    return self.probe(word_rep)\n",
    "\n",
    "random_model = LinearProbeRandom(len(label_vocab))\n",
    "with torch.no_grad():\n",
    "    y = random_model(torch.tensor([[0,1,2],[3,4,5]]).to(device))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 52])\n"
     ]
    }
   ],
   "source": [
    "class LinearProbeBert(nn.Module):\n",
    "  def __init__(self, num_labels):\n",
    "    super().__init__()\n",
    "    self.bert = AutoModel.from_pretrained('bert-base-cased')\n",
    "    self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    self.to(device)\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.probe.parameters()\n",
    "  \n",
    "  def forward(self, sentences):\n",
    "    with torch.no_grad():\n",
    "      word_rep, sentence_rep = self.bert(sentences, return_dict=False)\n",
    "    return self.probe(word_rep)\n",
    "\n",
    "bert_model = LinearProbeBert(len(label_vocab))\n",
    "y = bert_model(torch.tensor([[0, 1, 2], [3, 4, 5]]).to(device))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\ProbingBertPOS.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000013?line=37'>38</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m epoch, total_loss \u001b[39m/\u001b[39m num, \u001b[39m*\u001b[39mperf(model, eval_loader))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000013?line=40'>41</a>\u001b[0m \u001b[39m#fit(random_model, 5)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000013?line=41'>42</a>\u001b[0m fit(bert_model, \u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\ProbingBertPOS.ipynb Cell 11'\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000013?line=27'>28</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000013?line=28'>29</a>\u001b[0m total_loss \u001b[39m=\u001b[39m num \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000013?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000013?line=30'>31</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# start accumulating gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000013?line=31'>32</a>\u001b[0m   y_scores \u001b[39m=\u001b[39m model(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\ProbingBertPOS.ipynb Cell 8'\u001b[0m in \u001b[0;36mcollate_fn\u001b[1;34m(items)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollate_fn\u001b[39m(items):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=3'>4</a>\u001b[0m   max_len \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(item[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m items)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=5'>6</a>\u001b[0m   sentences \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros((\u001b[39mlen\u001b[39;49m(items), max_len), device\u001b[39m=\u001b[39;49mitems[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdevice)\u001b[39m.\u001b[39;49mlong()\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=6'>7</a>\u001b[0m   taggings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(items), max_len))\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=8'>9</a>\u001b[0m   \u001b[39mfor\u001b[39;00m i, (sentence, tagging) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(items):\n",
      "File \u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\env\\lib\\site-packages\\torch\\cuda\\__init__.py:210\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=205'>206</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=206'>207</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=207'>208</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=208'>209</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=209'>210</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=210'>211</a>\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=211'>212</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/User/Projects/Python/ProbingPretrainedLM/env/lib/site-packages/torch/cuda/__init__.py?line=212'>213</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def perf(model, loader):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  model.eval() # do not apply training-specific steps such as dropout\n",
    "  total_loss = correct = num_loss = num_perf = 0\n",
    "  for x, y in loader:\n",
    "    with torch.no_grad(): # no need to store computation graph for gradients\n",
    "      # perform inference and compute loss\n",
    "      y_scores = model(x)\n",
    "      loss = criterion(y_scores.view(-1, len(label_vocab)), y.view(-1)) # requires tensors of shape (num-instances, num-labels) and (num-instances)\n",
    "\n",
    "      # gather loss statistics\n",
    "      total_loss += loss.item()\n",
    "      num_loss += 1\n",
    "\n",
    "      # gather accuracy statistics\n",
    "      y_pred = torch.max(y_scores, 2)[1] # compute highest-scoring tag\n",
    "      mask = (y != 0) # ignore <pad> tags\n",
    "      correct += torch.sum((y_pred == y) * mask) # compute number of correct predictions\n",
    "      num_perf += torch.sum(mask).item()\n",
    "  return total_loss / num_loss, correct.item() / num_perf\n",
    "\n",
    "def fit(model, epochs):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = num = 0\n",
    "    for x, y in train_loader:\n",
    "      optimizer.zero_grad() # start accumulating gradients\n",
    "      y_scores = model(x)\n",
    "      loss = criterion(y_scores.view(-1, len(label_vocab)), y.view(-1))\n",
    "      loss.backward() # compute gradients though computation graph\n",
    "      optimizer.step() # modify model parameters\n",
    "      total_loss += loss.item()\n",
    "      num += 1\n",
    "    print(1 + epoch, total_loss / num, *perf(model, eval_loader))\n",
    "\n",
    "\n",
    "#fit(random_model, 5)\n",
    "fit(bert_model, 5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67741b0f2300521840e541af80430900906a19be91e702e560cd9f5360d1b38f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

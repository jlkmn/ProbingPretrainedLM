{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3vjsfPagUeJ",
        "outputId": "9e452294-1880-4a4a-98dd-31b0f312871e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# newdoc id = weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000\n",
            "# sent_id = weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000-0001\n",
            "# newpar id = weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000-p0001\n",
            "# text = Al-Zaman : American forces killed Shaikh Abdullah al-Ani, the preacher at the mosque in the town of Qaim, near the Syrian border.\n",
            "1\tAl\tAl\tPROPN\tNNP\tNumber=Sing\t0\troot\t0:root\tSpaceAfter=No\n",
            "2\t-\t-\tPUNCT\tHYPH\t_\t1\tpunct\t1:punct\tSpaceAfter=No\n",
            "3\tZaman\tZaman\tPROPN\tNNP\tNumber=Sing\t1\tflat\t1:flat\t_\n",
            "4\t:\t:\tPUNCT\t:\t_\t1\tpunct\t1:punct\t_\n",
            "5\tAmerican\tAmerican\tADJ\tJJ\tDegree=Pos\t6\tamod\t6:amod\t_\n",
            "6\tforces\tforce\tNOUN\tNNS\tNumber=Plur\t7\tnsubj\t7:nsubj\t_\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "for filename in ['en_ewt-ud-train.conllu', 'en_ewt-ud-dev.conllu', 'en_ewt-ud-test.conllu']:\n",
        "  urllib.request.urlretrieve('https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/' + filename, filename)\n",
        "\n",
        "with open('en_ewt-ud-train.conllu') as fp:\n",
        "  for line in fp.readlines()[:10]:\n",
        "    print(line, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpIdqj3vjje8",
        "outputId": "58a0cc4d-0403-4cc2-ed08-3be66c33f636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en_ewt-ud-test.conllu\n"
          ]
        }
      ],
      "source": [
        "print(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emn-nYc2ijCV",
        "outputId": "dbdb17b6-0091-4331-ee94-0e516dc14bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n"
          ]
        }
      ],
      "source": [
        "!pip -q install conllu\n",
        "\n",
        "import conllu\n",
        "\n",
        "def load_conllu(filename):\n",
        "  with open(filename) as fp:\n",
        "    data = conllu.parse(fp.read())\n",
        "  sentences = [[token['form'] for token in sentence] for sentence in data]\n",
        "  taggings = [[token['xpos'] for token in sentence] for sentence in data]\n",
        "\n",
        "  return sentences, taggings\n",
        "\n",
        "train_sentences, train_taggings = load_conllu('en_ewt-ud-train.conllu')\n",
        "valid_sentences, valid_taggings = load_conllu('en_ewt-ud-dev.conllu')\n",
        "test_sentences, test_taggings = load_conllu('en_ewt-ud-test.conllu')\n",
        "\n",
        "print(train_sentences[0])\n",
        "\n",
        "#print(list(zip(train_sentences[42], train_taggings[42])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp4_PN5uitkx",
        "outputId": "7cd2bc6f-b24d-4ddb-efd1-14325c3b6180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Gore', 'NNP'), ('released', 'VBD'), ('a', 'DT'), ('statement', 'NN'), ('Friday', 'NNP'), ('taking', 'VBG'), ('Bush', 'NNP'), ('to', 'IN'), ('task', 'NN'), ('for', 'IN'), ('his', 'PRP$'), ('comments', 'NNS'), ('on', 'IN'), (\"Pakistan's\", None), ('Pakistan', 'NNP'), (\"'s\", 'POS'), ('recent', 'JJ'), ('coup', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "print(list(zip(train_sentences[182], train_taggings[182])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRfJlioQmuly",
        "outputId": "a8e620f1-d120-4e66-eab6-b5953b5955cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN\n",
            "move\n",
            "number of different tags: 51\n",
            "26920 NN\n",
            "20888 IN\n",
            "16845 DT\n",
            "12401 NNP\n",
            "12220 PRP\n",
            "11588 JJ\n",
            "10592 RB\n",
            "10317 .\n",
            "9492 VB\n",
            "8450 NNS\n",
            "8062 ,\n",
            "6693 CC\n",
            "5407 VBD\n",
            "5356 VBP\n",
            "4572 VBZ\n",
            "4002 CD\n",
            "3962 VBN\n",
            "3330 VBG\n",
            "3293 MD\n",
            "3283 TO\n",
            "3044 PRP$\n",
            "2615 None\n",
            "1007 -RRB-\n",
            "973 -LRB-\n",
            "950 WDT\n",
            "867 :\n",
            "845 WRB\n",
            "813 ``\n",
            "785 ''\n",
            "761 WP\n",
            "758 RP\n",
            "696 UH\n",
            "684 POS\n",
            "661 HYPH\n",
            "573 NNPS\n",
            "511 JJR\n",
            "446 JJS\n",
            "361 EX\n",
            "338 NFP\n",
            "316 RBR\n",
            "292 ADD\n",
            "292 GW\n",
            "257 $\n",
            "175 PDT\n",
            "160 SYM\n",
            "117 LS\n",
            "101 RBS\n",
            "93 FW\n",
            "48 AFX\n",
            "15 WP$\n",
            "1 XX\n"
          ]
        }
      ],
      "source": [
        "# use a defaultdict to count the number of occurrences of each tag\n",
        "import collections\n",
        "tagset = collections.defaultdict(int)\n",
        "\n",
        "for idx, sentence in enumerate(train_sentences):\n",
        "  for idy, word in enumerate(sentence):\n",
        "    if(word==\"move\"):\n",
        "      break\n",
        "\n",
        "\n",
        "for tagging in train_taggings:\n",
        "  for tag in tagging:\n",
        "    tagset[tag] += 1\n",
        "\n",
        "print(train_taggings[857][24])\n",
        "\n",
        "print(train_sentences[857][24])\n",
        "print('number of different tags:', len(tagset))\n",
        "\n",
        "# print count and tag sorted by decreasing count\n",
        "for tag, count in sorted(tagset.items(), reverse=True, key=lambda x: x[1]):\n",
        "  print(count, tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "6KBx0P4VsDHi",
        "outputId": "c1581b5c-0d57-470c-f981-16c5b4a98479"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max length: 160\n",
            "avg length: 16.521406362114327\n",
            "longest sentence: ['Antichrist', 'John', 'Lennon', 'wanted', 'to', 'compete', 'with', 'Jesus', 'Christ', ',', 'and', 'so', 'he', 'grew', 'a', 'beard', 'and', 'started', 'to', 'make', 'a', 'bogus', 'role', 'of', 'Christ', 'together', 'with', 'Yoko', 'Ono', 'at', 'the', 'Amsterdam', 'Hilton', 'hotel', 'proclaiming', '\"', 'Peace', '\"', ',', 'being', 'then', 'when', 'he', 'was', 'visited', 'by', 'the', 'Canadian', 'journalist', 'who', 'ridiculized', 'and', 'admonished', 'him', 'wanting', 'to', 'know', 'about', 'what', 'Lennon', 'meant', 'when', 'he', 'wrote', 'in', 'the', 'lyrics', 'of', '\"', 'The', 'ballad', 'of', 'John', 'and', 'Yoko', '\"', ':', '\"', 'the', 'way', 'things', 'are', 'going', ',', \"they're\", 'they', \"'re\", 'going', 'to', 'crucify', 'me', '...', '\"', ',', 'The', 'CURSE', 'OF', 'GOD', 'upon', 'John', 'Lennon', 'carried', 'on', 'with', 'all', 'type', 'of', 'miseries', 'and', 'distresses', 'which', 'made', 'Lennon', 'give', 'the', 'interview', 'to', 'the', '\"', 'Rolling', 'Stone', '\"', 'magazine', '(', 'today', 'condensed', 'in', 'the', '\"', 'Lennon', 'remembers', '\"', 'book', ')', 'where', 'he', 'speaks', 'about', 'how', 'bad', 'thing', 'were', 'going', 'for', 'him', 'blaming', '\"', 'whatever', 'is', 'up', 'there', '\"', 'for', 'it', '(', 'referring', 'to', 'God', ')', '.']\n"
          ]
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# compute and show histogram for sentence length\n",
        "plt.hist([len(sentence) for sentence in train_sentences], 100)\n",
        "plt.show()\n",
        "\n",
        "# compute max sentence length\n",
        "print('max length:', max([len(sentence) for sentence in train_sentences]))\n",
        "\n",
        "# compute avg sentence length\n",
        "print('avg length:', sum([len(sentence) for sentence in train_sentences])/len([len(sentence) for sentence in train_sentences]))\n",
        "\n",
        "length_list=[len(sentence) for sentence in train_sentences]\n",
        "index = length_list.index(160)\n",
        "print(\"longest sentence:\", train_sentences[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgliLVKI1gSk"
      },
      "outputs": [],
      "source": [
        "# install transformers package\n",
        "!pip -q install transformers\n",
        "\n",
        "# import relevant classes for pretrained tokenizer and model\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru3xSNU11hjm",
        "outputId": "1069dea4-e20f-4f3b-a739-5ad13c210fbc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'token', '##izer', 'is', 'so', '##oo', '##oo', 'awesome', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# load tokenizer for a specific bert model (bert-base-cased)\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# tokenize an example sentence\n",
        "tokenizer.tokenize('This tokenizer is sooooo awesome.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVeKDlZZRN8A",
        "outputId": "b8ba3672-d929-4e7f-88f4-63db467676d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'has', 'been', 'talk', 'that', 'the', 'night', 'cu', '##rf', '##ew', 'might', 'be', 'implemented', 'again', '.']\n",
            "['EX', 'VBZ', 'VBN', 'NN', 'IN', 'DT', 'NN', '<pad>', '<pad>', 'NN', 'MD', 'VB', 'VBN', 'RB', '.']\n",
            "avg length difference: 3.385155066570995\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def align_tokenizations(sentences, taggings):\n",
        "  bert_tokenized_sentences = []\n",
        "  aligned_taggings = []\n",
        "\n",
        "  for sentence, tagging in zip(sentences, taggings):\n",
        "    # first generate BERT-tokenization\n",
        "    bert_tokenized_sentence = tokenizer.tokenize(' '.join(sentence))\n",
        "\n",
        "    aligned_tagging = []\n",
        "    current_word = ''\n",
        "    index = 0 # index of current word in sentence and tagging\n",
        "    for token in bert_tokenized_sentence:\n",
        "      current_word += re.sub(r'^##', '', token) # recompose word with subtoken\n",
        "      sentence[index] = sentence[index].replace('\\xad', '') # fix bug in data\n",
        "\n",
        "      # note that some word factors correspond to unknown words in BERT\n",
        "      assert token == '[UNK]' or sentence[index].startswith(current_word)\n",
        "\n",
        "      if token == '[UNK]' or sentence[index] == current_word: # if we completed a word\n",
        "        current_word = ''\n",
        "        aligned_tagging.append(tagging[index])\n",
        "        index += 1\n",
        "      else: # otherwise insert padding\n",
        "        aligned_tagging.append('<pad>')\n",
        "\n",
        "    assert len(bert_tokenized_sentence) == len(aligned_tagging)\n",
        "\n",
        "    bert_tokenized_sentences.append(bert_tokenized_sentence)\n",
        "    aligned_taggings.append(aligned_tagging)\n",
        "\n",
        "  return bert_tokenized_sentences, aligned_taggings\n",
        "\n",
        "train_bert_tokenized_sentences, train_aligned_taggings = align_tokenizations(train_sentences, train_taggings)\n",
        "valid_bert_tokenized_sentences, valid_aligned_taggings = align_tokenizations(valid_sentences, valid_taggings)\n",
        "test_bert_tokenized_sentences, test_aligned_taggings = align_tokenizations(test_sentences, test_taggings)\n",
        "\n",
        "\n",
        "\n",
        "print(train_bert_tokenized_sentences[42])\n",
        "print(train_aligned_taggings[42])\n",
        "print('avg length difference:', \n",
        "      sum([len(sentence) for sentence in train_bert_tokenized_sentences])/len([len(sentence) for sentence in train_bert_tokenized_sentences]) -\n",
        "      sum([len(sentence) for sentence in train_sentences])/len([len(sentence) for sentence in train_sentences])\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqVbzHtgRf8D",
        "outputId": "0aa130a6-453b-462e-886c-6f9ddec3565a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mosque', '##s', 'are', 'calling', 'for', 'don', '##ating', 'blood', ',', 'food', ',', 'and', 'medicine', 'for', 'Fall', '##u', '##jah', ',', 'and', 'several', 'convoys', 'have', 'already', 'headed', 'out', 'for', 'Fall', '##u', '##jah', ',', 'most', 'of', 'them', 'returned', 'later', 'though', '.']\n",
            "tensor([  101,  1247,  1144,  1151,  2037,  1115,  1103,  1480, 16408, 11931,\n",
            "         5773,  1547,  1129,  7042,  1254,   119,   100], device='cuda:0')\n",
            "tensor([ 0, 30, 22, 19,  9, 10,  8,  9,  0,  0,  9, 13, 14, 19, 23, 11,  0],\n",
            "       device='cuda:0')\n",
            "num labels: 47\n",
            "valid sentences [tensor([  101,  1622,  1103, 10997,  2502,  1142,  1642,   131,   100],\n",
            "       device='cuda:0')]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "\n",
        "import collections\n",
        "\n",
        "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
        "label_vocab['<pad>'] = 0\n",
        "\n",
        "def convert_to_ids(sentences, taggings):\n",
        "  sentences_ids = []\n",
        "  taggings_ids = []\n",
        "  for sentence, tagging in zip(sentences, taggings):\n",
        "    sentence_tensor = torch.tensor(tokenizer.convert_tokens_to_ids(['[CLS]'] + sentence + ['SEP'])).long()\n",
        "    tagging_tensor = torch.tensor([0] + [label_vocab[tag] for tag in tagging] + [0]).long()\n",
        "\n",
        "    for word in sentence: \n",
        "      if word == \"food\":\n",
        "        print(sentence)\n",
        "\n",
        "    sentences_ids.append(sentence_tensor.to(device))\n",
        "    taggings_ids.append(tagging_tensor.to(device))\n",
        "  return sentences_ids, taggings_ids\n",
        "\n",
        "train_sentences_ids, train_taggings_ids = convert_to_ids(train_bert_tokenized_sentences[:100], train_aligned_taggings[:100])\n",
        "valid_sentences_ids, valid_taggings_ids = convert_to_ids(valid_bert_tokenized_sentences[:100], valid_aligned_taggings[:100])\n",
        "test_sentences_ids, test_taggings_ids = convert_to_ids(test_bert_tokenized_sentences[:100], test_aligned_taggings[:100])\n",
        "\n",
        "print(train_sentences_ids[42])\n",
        "print(train_taggings_ids[42])\n",
        "print('num labels:', len(label_vocab))\n",
        "\n",
        "sentences_batch_ids = valid_sentences_ids[:1]\n",
        "taggings_batch_ids = valid_taggings_ids[:1]\n",
        "print(\"valid sentences\",valid_sentences_ids[:1])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"food id is: \", tokenizer.convert_tokens_to_ids(\"food\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GshQ2EkurnU1",
        "outputId": "e83d6c9e-29ec-44b0-f0e1-4ab3fd961306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "food id is:  2094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PosTaggingDataset(Dataset):\n",
        "  def __init__(self, sentences, taggings):\n",
        "    assert len(sentences) == len(taggings)\n",
        "    self.sentences = sentences\n",
        "    self.taggings = taggings\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return self.sentences[i], self.taggings[i]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentences)"
      ],
      "metadata": {
        "id": "J-vWkkcHteed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(items):\n",
        "  max_len = max(len(item[0]) for item in items)\n",
        "  print(max_len)\n",
        "\n",
        "  sentences = torch.zeros((len(items), max_len), device=items[0][0].device).long().to(device)\n",
        "\n",
        "  taggings = torch.zeros((len(items), max_len)).long().to(device)\n",
        "\n",
        "\n",
        "  for i, (sentence, tagging) in enumerate(items):\n",
        "    sentences[i][0:len(sentence)] = sentence\n",
        "    taggings[i][0:len(tagging)] = tagging\n",
        "\n",
        "  return sentences, taggings\n",
        "\n",
        "#sentence tag sentence tag\n",
        "#used to create batches as training set\n",
        "x, y = collate_fn([[torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6])]])\n",
        "print(x.shape, y.shape)\n",
        "big_tensor = [[torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6])]]\n",
        "\n",
        "print(big_tensor)\n",
        "other_tensor = [[sentences_batch_ids], [taggings_batch_ids]]\n",
        "\n",
        "\n",
        "#print(\"tag\", taggings_batch_ids.shape)\n",
        "#print(\"sen\", sentences_batch_ids.shape)\n",
        "#x2, y2 = collate_fn(other_tensor)\n",
        "#print(x2.shape, y2.shape)\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVQCQkrjubC9",
        "outputId": "a007b2c0-6a1e-4b00-d590-3f705b0ce153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "torch.Size([1, 3]) torch.Size([1, 3])\n",
            "[[tensor([1, 2, 3]), tensor([4, 5, 6])]]\n",
            "tensor([[4, 5, 6]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(PosTaggingDataset(train_sentences_ids, train_taggings_ids), batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "valid_loader = DataLoader(PosTaggingDataset(valid_sentences_ids, valid_taggings_ids), batch_size=batch_size, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(PosTaggingDataset(test_sentences_ids, test_taggings_ids), batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "batch_loader = DataLoader(PosTaggingDataset(sentences_batch_ids, taggings_batch_ids), batch_size=batch_size, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "djFAKpBEvCO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "postag = PosTaggingDataset(train_sentences_ids, train_taggings_ids)\n",
        "\n",
        "first, second = postag.__getitem__(1)\n",
        "print(first)\n",
        "print(first.shape)\n",
        "print(postag.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu2UWsMDwWrN",
        "outputId": "da094500-2d17-45b0-d2ec-ebe05b063c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 101,  164, 1188, 3646, 1104,  170, 9581,  172, 2879, 1596, 1209, 1129,\n",
            "        3989, 1366, 3819, 1111, 1201, 1106, 1435,  119,  166,  100],\n",
            "       device='cuda:0')\n",
            "torch.Size([22])\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinearProbeBert(nn.Module):\n",
        "  def __init__(self, num_labels):\n",
        "    super().__init__()\n",
        "    self.bert = AutoModel.from_pretrained('bert-base-cased')\n",
        "    self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "    self.to(device)\n",
        "\n",
        "  def parameters(self):\n",
        "    return self.probe.parameters()\n",
        "  \n",
        "  def forward(self, sentences):\n",
        "    with torch.no_grad(): # no training of BERT parameters\n",
        "      word_rep, sentence_rep = self.bert(sentences, return_dict=False)\n",
        "    return self.probe(word_rep)\n",
        "\n",
        "# the model should return a tensor of shape (batch size, sequence length, number of labels)\n",
        "bert_model = LinearProbeBert(len(label_vocab))\n",
        "y = bert_model(torch.tensor([[0, 1, 2], [3, 4, 5]]).to(device))\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142,
          "referenced_widgets": [
            "83b25f1f340542c392d1109cc3fc02be",
            "af051da166b5408b97262bfbb04868a5",
            "822dbefbb1844ced919108e57a2dd589",
            "35dc625b360a419884dd86d8ab7ac2e5",
            "d6de9c45875b489daefa0d3b4aea4944",
            "05cff01cc0d241d38a205e290e3ff0c9",
            "3320fa804e9547cf858f417fad816c83",
            "388800aac2f54058bef48e623777da04",
            "3790cb7327fd4c34b597df612ff2a496",
            "c0108378ccc8420fabad57495269cf7c",
            "a1dd00a3f04b408e86d91cb2b22b4324"
          ]
        },
        "id": "15FDIAe9vsXi",
        "outputId": "7ebab336-973b-4878-ee2f-4f1d46262ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83b25f1f340542c392d1109cc3fc02be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 47])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perf(model, loader):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model.eval() # do not apply training-specific steps such as dropout\n",
        "  total_loss = correct = num_loss = num_perf = 0\n",
        "  for x, y in loader:\n",
        "    with torch.no_grad(): # no need to store computation graph for gradients\n",
        "      # perform inference and compute loss\n",
        "      y_scores = model(x)\n",
        "      loss = criterion(y_scores.view(-1, len(label_vocab)), y.view(-1)) # requires tensors of shape (num-instances, num-labels) and (num-instances)\n",
        "\n",
        "      # gather loss statistics\n",
        "      total_loss += loss.item()\n",
        "      num_loss += 1\n",
        "\n",
        "      # gather accuracy statistics\n",
        "      y_pred = torch.max(y_scores, 2)[1] # compute highest-scoring tag\n",
        "      mask = (y != 0) # ignore <pad> tags\n",
        "      correct += torch.sum((y_pred == y) * mask) # compute number of correct predictions\n",
        "      num_perf += torch.sum(mask).item()\n",
        "  return total_loss / num_loss, correct.item() / num_perf\n",
        "\n",
        "# without training, accuracy should be a bit less than 2% (chance of getting a label correct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH_CUOUAycHj",
        "outputId": "e997e86d-19e0-4ea9-fc69-1bca23b026a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72\n",
            "51\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.804396629333496, 0.016999575010624733)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def fit(model, epochs):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = num = 0\n",
        "    for x, y in train_loader:\n",
        "      optimizer.zero_grad() # start accumulating gradients\n",
        "      y_scores = model(x)\n",
        "      loss = criterion(y_scores.view(-1, len(label_vocab)), y.view(-1))\n",
        "      loss.backward() # compute gradients though computation graph\n",
        "      optimizer.step() # modify model parameters\n",
        "      total_loss += loss.item()\n",
        "      num += 1\n",
        "    print(1 + epoch, total_loss / num, *perf(model, valid_loader))"
      ],
      "metadata": {
        "id": "G8t6jVSpORF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = LinearProbeBert(len(label_vocab))\n",
        "fit(bert_model, 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2JSXsziOk18",
        "outputId": "64b47fc6-b760-4dce-f846-b668e716f267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "1 2.533652365207672 1.3997892141342163 0.12919677008074798\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "2 1.0447359085083008 1.294036626815796 0.2562685932851679\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "3 1.0484595894813538 1.1569899320602417 0.35996600084997876\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "4 0.9161023795604706 1.0057126879692078 0.4878878028049299\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "5 0.7776782214641571 0.9119056165218353 0.6064598385040374\n",
            "91\n",
            "79\n",
            "72\n",
            "51\n",
            "6 0.6847628355026245 0.8558941185474396 0.6774330641733957\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "7 0.5956384241580963 0.8023844361305237 0.7016574585635359\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "8 0.5468049347400665 0.7497296631336212 0.7246068848278793\n",
            "79\n",
            "91\n",
            "72\n",
            "51\n",
            "9 0.47289836406707764 0.6923676431179047 0.7475563110922226\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "10 0.41475848853588104 0.6428478360176086 0.7586060348491288\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "11 0.3774401843547821 0.596400797367096 0.768380790480238\n",
            "82\n",
            "91\n",
            "72\n",
            "51\n",
            "12 0.3310454487800598 0.5488145053386688 0.7781555461113472\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "13 0.28785914182662964 0.5092784017324448 0.7866553336166596\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "14 0.2564849406480789 0.4770594835281372 0.7938801529961751\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "15 0.23484134674072266 0.45468655228614807 0.7968550786230344\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "16 0.22577285021543503 0.4385533183813095 0.798555036124097\n",
            "91\n",
            "70\n",
            "72\n",
            "51\n",
            "17 0.22280291467905045 0.42107367515563965 0.8032299192520187\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "18 0.17930582910776138 0.40562012791633606 0.8130046748831279\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "19 0.17555509507656097 0.3924948275089264 0.8176795580110497\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "20 0.1666998416185379 0.3804384618997574 0.8198045048873778\n",
            "91\n",
            "82\n",
            "72\n",
            "51\n",
            "21 0.15032555907964706 0.36991141736507416 0.8210794730131746\n",
            "91\n",
            "61\n",
            "72\n",
            "51\n",
            "22 0.15968014299869537 0.36031998693943024 0.8261793455163621\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "23 0.13158269971609116 0.3512590229511261 0.8274543136421589\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "24 0.12425969168543816 0.34396158158779144 0.8249043773905652\n",
            "91\n",
            "82\n",
            "72\n",
            "51\n",
            "25 0.12499827519059181 0.3384069502353668 0.8253293667658309\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "26 0.12078782171010971 0.33237098157405853 0.8278793030174245\n",
            "91\n",
            "78\n",
            "72\n",
            "51\n",
            "27 0.11432992294430733 0.32643045485019684 0.8317042073948151\n",
            "91\n",
            "82\n",
            "72\n",
            "51\n",
            "28 0.10051614418625832 0.3214036822319031 0.8359541011474713\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "29 0.10456429794430733 0.31759168207645416 0.8444538886527837\n",
            "91\n",
            "78\n",
            "72\n",
            "51\n",
            "30 0.10199453309178352 0.31433387100696564 0.8453038674033149\n",
            "91\n",
            "82\n",
            "72\n",
            "51\n",
            "31 0.09520763158798218 0.31009478867053986 0.8478538036549086\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "32 0.0930529348552227 0.3064813017845154 0.8427539311517213\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "33 0.08832225203514099 0.3031407743692398 0.8431789205269868\n",
            "91\n",
            "78\n",
            "72\n",
            "51\n",
            "34 0.08963878825306892 0.29948875308036804 0.8461538461538461\n",
            "91\n",
            "79\n",
            "72\n",
            "51\n",
            "35 0.08085770532488823 0.29640741646289825 0.8457288567785806\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "36 0.08618074283003807 0.294121190905571 0.8470038249043774\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "37 0.08111037313938141 0.29241904616355896 0.8453038674033149\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "38 0.08023256808519363 0.2916955053806305 0.8457288567785806\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "39 0.07695505768060684 0.29091422259807587 0.8465788355291117\n",
            "91\n",
            "82\n",
            "72\n",
            "51\n",
            "40 0.07412898913025856 0.28899742662906647 0.8461538461538461\n",
            "91\n",
            "79\n",
            "72\n",
            "51\n",
            "41 0.07450089603662491 0.28690020740032196 0.847428814279643\n",
            "91\n",
            "82\n",
            "72\n",
            "51\n",
            "42 0.07307058572769165 0.2849949896335602 0.8487037824054399\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "43 0.07091042585670948 0.2835838943719864 0.8491287717807054\n",
            "91\n",
            "83\n",
            "72\n",
            "51\n",
            "44 0.07387812435626984 0.2828648090362549 0.8487037824054399\n",
            "91\n",
            "79\n",
            "72\n",
            "51\n",
            "45 0.06741862371563911 0.2818377763032913 0.847428814279643\n",
            "82\n",
            "91\n",
            "72\n",
            "51\n",
            "46 0.06666740775108337 0.2805628925561905 0.8487037824054399\n",
            "91\n",
            "82\n",
            "72\n",
            "51\n",
            "47 0.06322440318763256 0.2789602428674698 0.8512537186570336\n",
            "83\n",
            "91\n",
            "72\n",
            "51\n",
            "48 0.06447632610797882 0.2775883227586746 0.852953676158096\n",
            "91\n",
            "82\n",
            "72\n",
            "51\n",
            "49 0.06341436132788658 0.27680477499961853 0.8559286017849553\n",
            "91\n",
            "78\n",
            "72\n",
            "51\n",
            "50 0.06040811911225319 0.27620649337768555 0.8576285592860179\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "probing_bert_models_with_part_of_speech_tagging.ipynb",
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83b25f1f340542c392d1109cc3fc02be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af051da166b5408b97262bfbb04868a5",
              "IPY_MODEL_822dbefbb1844ced919108e57a2dd589",
              "IPY_MODEL_35dc625b360a419884dd86d8ab7ac2e5"
            ],
            "layout": "IPY_MODEL_d6de9c45875b489daefa0d3b4aea4944"
          }
        },
        "af051da166b5408b97262bfbb04868a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05cff01cc0d241d38a205e290e3ff0c9",
            "placeholder": "​",
            "style": "IPY_MODEL_3320fa804e9547cf858f417fad816c83",
            "value": "Downloading: 100%"
          }
        },
        "822dbefbb1844ced919108e57a2dd589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_388800aac2f54058bef48e623777da04",
            "max": 435779157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3790cb7327fd4c34b597df612ff2a496",
            "value": 435779157
          }
        },
        "35dc625b360a419884dd86d8ab7ac2e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0108378ccc8420fabad57495269cf7c",
            "placeholder": "​",
            "style": "IPY_MODEL_a1dd00a3f04b408e86d91cb2b22b4324",
            "value": " 416M/416M [00:08&lt;00:00, 55.1MB/s]"
          }
        },
        "d6de9c45875b489daefa0d3b4aea4944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05cff01cc0d241d38a205e290e3ff0c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3320fa804e9547cf858f417fad816c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "388800aac2f54058bef48e623777da04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3790cb7327fd4c34b597df612ff2a496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0108378ccc8420fabad57495269cf7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1dd00a3f04b408e86d91cb2b22b4324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
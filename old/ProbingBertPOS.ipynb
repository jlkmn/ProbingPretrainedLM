{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probing Bert part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "TRAIN_FILE = \"en_ewt-ud-train.conllu\"\n",
    "EVAL_FILE = \"en_ewt-ud-dev.conllu\"\n",
    "TEST_FILE = \"en_ewt-ud-test.conllu\"\n",
    "\n",
    "for filename in [ TRAIN_FILE, EVAL_FILE, TEST_FILE ]:\n",
    "  urllib.request.urlretrieve('https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/' + filename, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Al', 'NNP'), ('-', 'HYPH'), ('Zaman', 'NNP'), (':', ':'), ('American', 'JJ'), ('forces', 'NNS'), ('killed', 'VBD'), ('Shaikh', 'NNP'), ('Abdullah', 'NNP'), ('al', 'NNP'), ('-', 'HYPH'), ('Ani', 'NNP'), (',', ','), ('the', 'DT'), ('preacher', 'NN'), ('at', 'IN'), ('the', 'DT'), ('mosque', 'NN'), ('in', 'IN'), ('the', 'DT'), ('town', 'NN'), ('of', 'IN'), ('Qaim', 'NNP'), (',', ','), ('near', 'IN'), ('the', 'DT'), ('Syrian', 'JJ'), ('border', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import conllu\n",
    "\n",
    "def load_conllu(filename):\n",
    "  with open(filename, encoding=\"utf-8\") as fp:\n",
    "    data = conllu.parse(fp.read())\n",
    "  sentences = [[token['form'] for token in sentence] for sentence in data]\n",
    "  taggings = [[token['xpos'] for token in sentence] for sentence in data]\n",
    "  return sentences, taggings\n",
    "\n",
    "train_sentences, train_labels = load_conllu(TRAIN_FILE)\n",
    "eval_sentences, eval_labels = load_conllu(EVAL_FILE)\n",
    "test_sentences, test_labels = load_conllu(TEST_FILE)\n",
    "print(list(zip(train_sentences[0], train_labels[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'interesting',\n",
       " 'for',\n",
       " 'su',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##u',\n",
       " '##ure']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.tokenize(\"This is interesting for suuuuuuuuuuuuuuuuuure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'has', 'been', 'talk', 'that', 'the', 'night', 'cu', '##rf', '##ew', 'might', 'be', 'implemented', 'again', '.']\n",
      "['EX', 'VBZ', 'VBN', 'NN', 'IN', 'DT', 'NN', '<pad>', '<pad>', 'NN', 'MD', 'VB', 'VBN', 'RB', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def align_to_bert_tokenization(sentences, labels):\n",
    "    tokenized_sentences = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    for s, l in zip(sentences, labels):\n",
    "        tokenized_sentence = tokenizer.tokenize(' '.join(s))\n",
    "        aligned_label = []\n",
    "        current_word = ''\n",
    "        i = 0\n",
    "        for token in tokenized_sentence:\n",
    "            current_word += re.sub(r'^##', '', token)\n",
    "            s[i] = s[i].replace('\\xad', '')\n",
    "            \n",
    "            assert token == '[UNK]' or s[i].startswith(current_word)\n",
    "\n",
    "            if token == '[UNK]' or s[i] == current_word:\n",
    "                current_word = ''\n",
    "                aligned_label.append(l[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                aligned_label.append('<pad>')\n",
    "        \n",
    "        assert len(tokenized_sentence) == len(aligned_label)\n",
    "\n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "        aligned_labels.append(aligned_label)\n",
    "    \n",
    "    return tokenized_sentences, aligned_labels\n",
    "\n",
    "\n",
    "train_bert_tokenized_sentences, train_aligned_taggings = align_to_bert_tokenization(train_sentences, train_labels)\n",
    "eval_bert_tokenized_sentences, valid_aligned_taggings = align_to_bert_tokenization(eval_sentences, eval_labels)\n",
    "test_bert_tokenized_sentences, test_aligned_taggings = align_to_bert_tokenization(test_sentences, test_labels)\n",
    "\n",
    "print(train_bert_tokenized_sentences[42])\n",
    "print(train_aligned_taggings[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1247,  1144,  1151,  2037,  1115,  1103,  1480, 16408, 11931,\n",
      "         5773,  1547,  1129,  7042,  1254,   119,   100])\n",
      "tensor([ 0, 30, 22, 19,  9, 10,  8,  9,  0,  0,  9, 13, 14, 19, 23, 11,  0])\n",
      "num labels: 52\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')\n",
    "\n",
    "import collections\n",
    "\n",
    "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
    "label_vocab['<pad>'] = 0\n",
    "\n",
    "def convert_to_ids(sentences, taggings):\n",
    "  sentences_ids = []\n",
    "  taggings_ids = []\n",
    "  for sentence, tagging in zip(sentences, taggings):\n",
    "    sentence_tensor = torch.tensor(tokenizer.convert_tokens_to_ids(['[CLS]'] + sentence + ['SEP'])).long()\n",
    "    tagging_tensor = torch.tensor([0] + [label_vocab[tag] for tag in tagging] + [0]).long()\n",
    "\n",
    "    sentences_ids.append(sentence_tensor.to(device))\n",
    "    taggings_ids.append(tagging_tensor.to(device))\n",
    "  return sentences_ids, taggings_ids\n",
    "\n",
    "train_sentences_ids, train_taggings_ids = convert_to_ids(train_bert_tokenized_sentences, train_aligned_taggings)\n",
    "eval_sentences_ids, eval_taggings_ids = convert_to_ids(eval_bert_tokenized_sentences, valid_aligned_taggings)\n",
    "test_sentences_ids, test_taggings_ids = convert_to_ids(test_bert_tokenized_sentences, test_aligned_taggings)\n",
    "\n",
    "print(train_sentences_ids[42])\n",
    "print(train_taggings_ids[42])\n",
    "print('num labels:', len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PosTaggingDataset(Dataset):\n",
    "  def __init__(self, sentences, taggings):\n",
    "    assert len(sentences) == len(taggings)\n",
    "    self.sentences = sentences\n",
    "    self.taggings = taggings\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    return self.sentences[i], self.taggings[i]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(items):\n",
    "  max_len = max(len(item[0]) for item in items)\n",
    "\n",
    "  sentences = torch.zeros((len(items), max_len), device=items[0][0].device).long().to(device)\n",
    "  taggings = torch.zeros((len(items), max_len)).long().to(device)\n",
    "\n",
    "  for i, (sentence, tagging) in enumerate(items):\n",
    "    sentences[i][0:len(sentence)] = sentence\n",
    "    taggings[i][0:len(tagging)] = tagging\n",
    "\n",
    "  return sentences, taggings\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(PosTaggingDataset(train_sentences_ids, train_taggings_ids), batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "eval_loader = DataLoader(PosTaggingDataset(eval_sentences_ids, eval_taggings_ids), batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(PosTaggingDataset(test_sentences_ids, test_taggings_ids), batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 52])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearProbeRandom(nn.Module):\n",
    "  def __init__(self, num_labels):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(tokenizer.vocab_size, 768)\n",
    "    self.probe = nn.Linear(768, num_labels)\n",
    "    self.to(device)\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.probe.parameters()\n",
    "  \n",
    "  def forward(self, sentences):\n",
    "    with torch.no_grad():\n",
    "      word_rep = self.embedding(sentences)\n",
    "    return self.probe(word_rep)\n",
    "\n",
    "random_model = LinearProbeRandom(len(label_vocab))\n",
    "with torch.no_grad():\n",
    "    y = random_model(torch.tensor([[0,1,2],[3,4,5]]).to(device))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 52])\n"
     ]
    }
   ],
   "source": [
    "class LinearProbeBert(nn.Module):\n",
    "  def __init__(self, num_labels):\n",
    "    super().__init__()\n",
    "    self.bert = AutoModel.from_pretrained('bert-base-cased')\n",
    "    self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    self.to(device)\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.probe.parameters()\n",
    "  \n",
    "  def forward(self, sentences):\n",
    "    with torch.no_grad():\n",
    "      word_rep, sentence_rep = self.bert(sentences, return_dict=False)\n",
    "    return self.probe(word_rep)\n",
    "\n",
    "bert_model = LinearProbeBert(len(label_vocab))\n",
    "y = bert_model(torch.tensor([[0, 1, 2], [3, 4, 5]]).to(device))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 72, 52])\n",
      "torch.Size([64, 72])\n",
      "tensor([[  5.3464,  -1.3886,  -7.3429,  ..., -10.1562,  -8.8742, -12.8704],\n",
      "        [  3.7534,   1.7354,  -3.7966,  ...,  -2.3750,  -2.5712,  -8.7468],\n",
      "        [  4.8144,   1.6469,  -4.1465,  ...,  -1.2046,  -6.7123, -11.4561],\n",
      "        ...,\n",
      "        [  5.8665,  -3.7160, -11.7380,  ..., -16.1012, -10.4523, -19.0601],\n",
      "        [  5.8665,  -3.7160, -11.7380,  ..., -16.1012, -10.4523, -19.0601],\n",
      "        [  5.8665,  -3.7160, -11.7380,  ..., -16.1012, -10.4523, -19.0601]])\n",
      "tensor([ 0, 10,  8,  ...,  0,  0,  0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\ProbingBertPOS.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=42'>43</a>\u001b[0m       num \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=43'>44</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m epoch, total_loss \u001b[39m/\u001b[39m num, \u001b[39m*\u001b[39mperf(model, eval_loader))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=46'>47</a>\u001b[0m fit(random_model, \u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\ProbingBertPOS.ipynb Cell 11'\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=41'>42</a>\u001b[0m   total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=42'>43</a>\u001b[0m   num \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m epoch, total_loss \u001b[39m/\u001b[39m num, \u001b[39m*\u001b[39mperf(model, eval_loader))\n",
      "\u001b[1;32mc:\\Users\\User\\Projects\\Python\\ProbingPretrainedLM\\ProbingBertPOS.ipynb Cell 11'\u001b[0m in \u001b[0;36mperf\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_scores\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(label_vocab)), y\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m# requires tensors of shape (num-instances, num-labels) and (num-instances)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=18'>19</a>\u001b[0m \u001b[39m# gather loss statistics\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=19'>20</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=20'>21</a>\u001b[0m num_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Projects/Python/ProbingPretrainedLM/ProbingBertPOS.ipynb#ch0000010?line=22'>23</a>\u001b[0m \u001b[39m# gather accuracy statistics\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def perf(model, loader):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  model.eval() # do not apply training-specific steps such as dropout\n",
    "  total_loss = correct = num_loss = num_perf = 0\n",
    "  for x, y in loader:\n",
    "    with torch.no_grad(): # no need to store computation graph for gradients\n",
    "      # perform inference and compute loss\n",
    "      y_scores = model(x)\n",
    "      print(y_scores.shape)\n",
    "      print(y.shape)\n",
    "      one = y_scores.view(-1, len(label_vocab))\n",
    "      two = y.view(-1)\n",
    "      print(one)\n",
    "      print(two)\n",
    "      loss = criterion(y_scores.view(-1, len(label_vocab)), y.view(-1)) # requires tensors of shape (num-instances, num-labels) and (num-instances)\n",
    "\n",
    "      # gather loss statistics\n",
    "      total_loss += loss.item()\n",
    "      num_loss += 1\n",
    "\n",
    "      # gather accuracy statistics\n",
    "      y_pred = torch.max(y_scores, 2)[1] # compute highest-scoring tag\n",
    "      mask = (y != 0) # ignore <pad> tags\n",
    "      correct += torch.sum((y_pred == y) * mask) # compute number of correct predictions\n",
    "      num_perf += torch.sum(mask).item()\n",
    "  return total_loss / num_loss, correct.item() / num_perf\n",
    "\n",
    "def fit(model, epochs):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = num = 0\n",
    "    for x, y in train_loader:\n",
    "      optimizer.zero_grad() # start accumulating gradients\n",
    "      y_scores = model(x)\n",
    "      loss = criterion(y_scores.view(-1, len(label_vocab)), y.view(-1))\n",
    "      loss.backward() # compute gradients though computation graph\n",
    "      optimizer.step() # modify model parameters\n",
    "      total_loss += loss.item()\n",
    "      num += 1\n",
    "    print(1 + epoch, total_loss / num, *perf(model, eval_loader))\n",
    "\n",
    "\n",
    "fit(random_model, 5)\n",
    "#fit(bert_model, 5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67741b0f2300521840e541af80430900906a19be91e702e560cd9f5360d1b38f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
